import json
import multiprocessing as mp
import os
import shutil
import subprocess
import sys
from pathlib import Path
from time import sleep
from typing import List

import requests  # type: ignore

from taipy.airflow import to_dag
from taipy.config import Config
from taipy.task import Task


class Airflow:
    """
    Interface with Airflow through API, CLI and disk.

    Dag generation are handle by the `to_dag` script that will convert JSON generated by this class
    to Airflow DAGs. The Config own the target of the disk that will hold JSONs and the convertor.

    Taipy trigger DAGs through the Airflow webserver. The Airflow auth should be set to `basic_auth`
    and the password of the Admin account should be put inside of Airflow Home set in the Config.

    In dev mode, the application can start an Airflow scheduler and webserver thanks to the
    `airflow standalone` command line. Airflow will run in background and the other part of the code
    will stay the same.
    """

    def __init__(self):
        self.airflow = None

    def __del__(self):
        self.stop()

    @staticmethod
    def can_start_airflow() -> bool:
        """True if the configuration allow the application to start Airflow itself"""
        return Config.job_config().start_airflow  # type: ignore

    @property
    def hostname(self) -> str:
        """Hostname of the Airflow webserver"""
        return Config.job_config().hostname  # type: ignore

    @property
    def airflow_folder(self) -> Path:
        """Airflow home folder where pid of the webserver is written"""
        return Path(Config.job_config().airflow_folder)  # type: ignore

    @property
    def dag_folder(self) -> Path:
        """Dag folder where the application will move DAGs and convertor"""
        return Path(Config.job_config().airflow_dag_folder)  # type: ignore

    def stop(self):
        """Stop Airflow process if exists"""
        if self.airflow:
            self.airflow.kill()

    def start(self):
        """
        Init and run Airflow in background and in the `standalone` mode.

        This function check if the file that contains the PID of the Airflow Webserver exists.
        If exists, nothing is done, else we init the Airflow Database then start the Webserver and Scheduler.
        """
        self._create_airflow_folder()
        self._add_dag_folder()
        self._add_json_to_airflow_convertor()

        if not self.is_running() and self.can_start_airflow():
            self._start_airflow()

    def is_running(self) -> bool:
        """
        Airflow Webserver writes its PID in a file when is it ready.
        Checking if exist allow us to know if Airflow is already running or not.

        Return:
            True if the file exist and so, an Airflow process is running
        """
        pid_file = self.airflow_folder / "airflow-webserver.pid"

        return pid_file.exists()

    def trigger(self, dag_id: str, tasks: List[Task]):
        """
        Unpaused and trigger a Dag through the Airflow REST API.
        """
        self.start()
        self._create_dag(dag_id, tasks)
        self._wait_dag_exists(dag_id)
        self._unpaused(dag_id)
        self._trigger(dag_id)

    def _create_dag(self, dag_id, tasks):
        json_model = {
            "path": sys.path[0],
            "dag_id": dag_id,
            "storage_folder": str(Path(Config.global_config().storage_folder).resolve()),  # type: ignore
            "tasks": [task.id for task in tasks],
        }
        dag_path = Path(Config.job_config().airflow_dag_folder).resolve() / "taipy" / f"{dag_id}.json"  # type: ignore

        dag_path.write_text(json.dumps(json_model))

    def _create_airflow_folder(self):
        os.makedirs(self.airflow_folder, exist_ok=True)

    def _wait_dag_exists(self, dag_id: str):
        endpoint = f"{self.hostname}/api/v1/dags/{dag_id}"
        auth = self._get_credentials()

        dag_exist = self._retry_on_airflow(lambda: requests.get(endpoint, auth=auth, json=None).status_code == 200)

        if not dag_exist:
            raise RuntimeError(f"Dag {dag_id} doesn't exist")

    def _unpaused(self, dag_id: str):
        requests.patch(f"{self.hostname}/api/v1/dags/{dag_id}", auth=self._get_credentials(), json={"is_paused": False})

    def _trigger(self, dag_id: str):
        requests.post(f"{self.hostname}/api/v1/dags/{dag_id}/dagRuns", auth=self._get_credentials(), json={})

    def _start_airflow(self):
        stdout_file = self.airflow_folder / "stdout"
        stderr_file = self.airflow_folder / "stderr"

        self.airflow = mp.Process(
            target=self._start_airflow_in_standalone_mode,
            args=(self.airflow_folder, self.dag_folder, stdout_file, stderr_file),
        )
        self.airflow.start()
        self._retry_on_airflow(self.is_running)

    def _add_dag_folder(self):
        taipy_dag_folder = self.dag_folder / "taipy"

        self.dag_folder.mkdir(exist_ok=True)
        taipy_dag_folder.mkdir(exist_ok=True)

    def _add_json_to_airflow_convertor(self):
        shutil.copy(to_dag.__file__, self.dag_folder)

    @staticmethod
    def _start_airflow_in_standalone_mode(airflow_folder, airflow_dag_folder, stdout_file, stderr_file):
        """
        Start Airflow in the Standalone mode through its CLI interface.
        Stdout and Stderr are redirects in two different files to keep logs and limit the
        pollution on the Taipy output.
        The connection on the Airflow webserver will be through the basic_auth, we set it through the environment.
        """
        os.environ["AIRFLOW_HOME"] = str(airflow_folder)
        os.environ["AIRFLOW__CORE__DAGS_FOLDER"] = str(airflow_dag_folder)
        os.environ["AIRFLOW__CORE__LOAD_EXAMPLES"] = str(False)
        os.environ["AIRFLOW__API__AUTH_BACKEND"] = "airflow.api.auth.backend.basic_auth"

        with open(stdout_file, "w") as stdout, open(stderr_file, "w") as stderr:
            subprocess.run(["airflow", "standalone"], stdout=stdout, stderr=stderr)

    def _get_credentials(self):
        password_file = self.airflow_folder / "standalone_admin_password.txt"

        return "admin", password_file.read_text()

    @staticmethod
    def _retry_on_airflow(cb):
        nb_retry = Config.job_config().airflow_api_retry

        for i in range(nb_retry):
            try:
                if cb():
                    return True
            except Exception as e:
                print("Ignoring exception:", e)
            sleep(1)
        return False
